# -*- coding: utf-8 -*-
"""Customer Churn Analysis on Dataset 2 - Telecommunication Dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18STPkAYG5CcPxtEm03QzRJUriK9i-7JE
"""

# import required libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn
import tensorflow as tf
tf.compat.v1.disable_v2_behavior() 
from tensorflow import keras
from tensorflow.keras.optimizers import Adam
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Dropout
from keras.layers import Lambda
from keras.layers import Activation

!unzip customer-churn-prediction-2020.zip

df = pd.read_csv("/content/train.csv")
df_test = pd.read_csv("/content/test.csv")

"""# DATA PREPROCESSING"""

# dropping Customer ID Column
df.drop(["state", "account_length"], axis=1, inplace=True)
df_test.drop(["state", "account_length"], axis=1, inplace=True)

# Impue NULL valued examples from the dataset
df.isnull().sum()

df.info()

"""1. area_code - obj
2. international_plan - obj
3. voice_mail_plan - obj
4. churn - obj

We need to encode these into numerical data
"""

df['area_code'].value_counts()

from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(sparse = False)
df[enc.categories_[0]] = enc.fit_transform(np.array(df['area_code']).reshape(-1, 1))
df_test[enc.categories_[0]] = enc.fit_transform(np.array(df_test['area_code']).reshape(-1, 1))

df.drop(['area_code'], axis = 1, inplace = True)
df_test.drop(['area_code'], axis = 1, inplace = True)

df['international_plan'] = (df['international_plan']=='yes').astype(float)
df_test['international_plan'] = (df_test['international_plan']=='yes').astype(float)
df['voice_mail_plan'] = (df['voice_mail_plan']=='yes').astype(float)
df_test['voice_mail_plan'] = (df_test['voice_mail_plan']=='yes').astype(float)

X = df.drop(['churn'], axis = 1)
y = df['churn']

"""# HANDLING CLASS IMBALANCES"""

from imblearn.over_sampling import SMOTE
oversample = SMOTE()
X, y = oversample.fit_resample(X, y)

y.value_counts()

"""# CREATE THE TRAIN AND TEST SETS"""

y = y.apply(lambda x:(x=='yes')).astype(float)

# Perform the Train-Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8,random_state=99)

X_train.shape

y_train

"""# DEEP FEED-FORWARD NEURAL NETWORK"""

model = keras.Sequential(
    [
        Dense(32, input_dim=19, activation="relu"),
        # Dropout(0.2),
        Dense(16, activation="relu"),
        # Dropout(0.3),
        Dense(8, activation="relu"),
        # Dropout(0.5),
        Dense(1, activation="sigmoid")
    ]
)

model.summary()

# compile the model

model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["binary_accuracy"])

# finally fit the model on the data

hist = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=15
)

"""# SVM CLASSIFIER"""

from sklearn import svm
clf = svm.SVC(C=100, kernel="poly", verbose=True, degree=5)
clf.fit(X_train, y_train)

clf.predict(X_test)

"""#EVALUATIONS"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, ConfusionMatrixDisplay

y_train_pred = [round(y_pred[0]) for y_pred in model.predict(X_train)]
y_test_pred = [round(y_pred[0]) for y_pred in model.predict(X_test)]

print("ACCURACY ON TRAIN SET: ", accuracy_score(y_train, y_train_pred))
print("ACCURACY ON TEST SET: ", accuracy_score(y_test, y_test_pred))

conf_mat = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)
disp.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test, y_test_pred))

y_train_pred = [int(y_pred>0.5) for y_pred in clf.predict(X_train)]
y_test_pred = [int(y_pred>0.5) for y_pred in clf.predict(X_test)]

print("ACCURACY ON TRAIN SET: ", accuracy_score(y_train, y_train_pred))
print("ACCURACY ON TEST SET: ", accuracy_score(y_test, y_test_pred))

conf_mat = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)
disp.plot()
plt.show()

print(classification_report(y_test, y_test_pred))

"""# SHAP EXPLANATIONS"""

!pip install shap

import shap

# SHAP expects model functions to take a 2D numpy array as input, 
# so we define a wrapper function around the original Keras predict function

def prediction_wrapper(X):
  return model.predict([X[:,i] for i in range(X.shape[0])]).flatten()

# select a set of background examples to take an expectation over
background = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False)]

explainer = shap.DeepExplainer(
    (model.layers[0].input, model.layers[-1].output), background
)
shap_values = explainer.shap_values(X_test[:].values) 

# print the JS visualization code to the notebook

shap.initjs()
shap.force_plot(
    explainer.expected_value[0], shap_values[0], feature_names=X_train.columns
)

# Negative Sample
shap.initjs()
shap.force_plot(
    explainer.expected_value[0], shap_values[0][1], feature_names=X_train.columns
)

# Positive Sample
shap.initjs()
shap.force_plot(
    explainer.expected_value[0], shap_values[0][470], feature_names=X_train.columns
)

shap.summary_plot(shap_values[0],X_test)
# shap_values

"""# PDP EXPLANATIONS"""

!pip install pdpbox

X.info()

from pdpbox import pdp, info_plots

features = ["international_plan", "voice_mail_plan", "number_vmail_messages", "total_day_minutes", "total_day_calls", "total_day_charge", "total_eve_minutes", "total_eve_calls", "total_eve_charge", "total_night_minutes", "total_night_calls", "total_night_charge", "total_intl_minutes", "total_intl_calls", "total_intl_charge", "number_customer_service_calls", "area_code_408", "area_code_415", "area_code_510"]

for feature_name in features:
    # Create the data that we will plot
    pdp_goals = pdp.pdp_isolate(model=model, dataset=X_test, model_features=X_train.columns.tolist(), feature=feature_name);

    # plot it
    pdp.pdp_plot(pdp_goals, feature_name);
    plt.show();

"""# CAUSAL ANALYSIS"""

!pip install cdt

!sudo add-apt-repository ppa:dns/gnu
!sudo apt-get update
!sudo apt install libgsl-dev

!Rscript setup.r

df = pd.read_csv("/content/train.csv")
df.head()

# drop all categorical features and convert non-numeric features to numeric
df.drop(['state', 'area_code', 'voice_mail_plan', 'international_plan'], axis=1, inplace = True)
df['churn'] = (df['churn']=='yes').astype(int)

import pickle
import cdt
import networkx as nx
import matplotlib.pyplot as plt

cdt.SETTINGS.rpath = "/usr/bin/Rscript"
cdt.utils.R.DefaultRPackages.pcalg = True
cdt.utils.R.DefaultRPackages.kpcalg = True
cdt.utils.R.DefaultRPackages.RCIT = True

# Estimated Causality Graph

# LiNGAM Algorithm
model_lingam = cdt.causality.graph.LiNGAM()
graph_lingam = model_lingam.predict(df)

# visualize network
fig=plt.figure(figsize=(25,20))
nx.draw_networkx(graph_lingam, font_size=18, font_color='r')

