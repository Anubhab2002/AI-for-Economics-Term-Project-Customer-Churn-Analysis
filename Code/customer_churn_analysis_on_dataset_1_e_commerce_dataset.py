# -*- coding: utf-8 -*-
"""Customer Churn Analysis on Dataset 1 - E-Commerce Dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v7X731khfV3tcvI9hVBYtS4oYBqoo6Up
"""

# import required libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn
import tensorflow as tf
tf.compat.v1.disable_v2_behavior() 
from tensorflow import keras
from tensorflow.keras.optimizers import Adam
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Dropout
from keras.layers import Lambda
from keras.layers import Activation

df = pd.read_excel("/content/E Commerce Dataset.xlsx", sheet_name = "E Comm")
df.head()

"""# DATA PREPROCESSING"""

# dropping Customer ID Column
df.drop("CustomerID", axis=1, inplace=True)

# Impue NULL valued examples from the dataset
df.isnull().sum()

df.info()

"""1. PreferredLogInDevice - obj
2. PreferredPaymentMode - obj
3. Gender - obj
4. PreferedOrderCat - obj
5. MaritalStatus - obj

We need to encode these into numerical data
"""

# separate out the Numerical and Categorical Columns to perform imputation on
categorical_cols = ['PreferredLoginDevice', 'PreferredPaymentMode', 'Gender', 'PreferedOrderCat', 'MaritalStatus']

numerical_cols = ['Churn', 'Tenure',  'CityTier',
       'WarehouseToHome',  'HourSpendOnApp',
       'NumberOfDeviceRegistered',  'SatisfactionScore',
        'NumberOfAddress', 'Complain',
       'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount',
       'DaySinceLastOrder', 'CashbackAmount']

from sklearn.impute import SimpleImputer
mean_imputer = SimpleImputer(missing_values = np.nan, strategy = "mean")
df[numerical_cols] = mean_imputer.fit_transform(df[numerical_cols])

df.isnull().sum()

# We will encode the catagorical variables using One-Hot Encoding

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(handle_unknown="ignore", sparse=False)
encoder.fit(df[categorical_cols])
encoded_cols = list(encoder.get_feature_names(categorical_cols))
print(encoded_cols)
df[encoded_cols] = encoder.transform(df[categorical_cols])

# Now we form the feature matrix and the target variable column vector from df

X = df[numerical_cols+encoded_cols]
X.drop("Churn", axis=1, inplace=True)

y = df["Churn"]

y.value_counts()

"""# HANDLING CLASS IMBALANCES"""

from imblearn.over_sampling import SMOTE
oversample = SMOTE()
X, y = oversample.fit_resample(X, y)

y.value_counts()

"""# CREATE THE TRAIN AND TEST SETS"""

# Perform the Train-Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8,random_state=99)

X_train.shape

y_train.value_counts()

"""# DEEP FEED-FORWARD NEURAL NETWORK"""

model = keras.Sequential(
    [
        Dense(16, input_dim=34, activation="relu"),
        Dense(8, activation="relu"),
        Dense(1, activation="sigmoid")
    ]
)

model.summary()

# compile the model

model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["binary_accuracy"])

# finally fit the model on the data

hist = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=15
)

"""# SVM CLASSIFIER"""

from sklearn import svm
clf = svm.SVC(C=100, kernel="poly", verbose=True, degree=2)
clf.fit(X_train, y_train)

clf.predict(X_test)

"""#EVALUATIONS"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, ConfusionMatrixDisplay

y_train_pred = [round(y_pred[0]) for y_pred in model.predict(X_train)]
y_test_pred = [round(y_pred[0]) for y_pred in model.predict(X_test)]

print("ACCURACY ON TRAIN SET: ", accuracy_score(y_train, y_train_pred))
print("ACCURACY ON TEST SET: ", accuracy_score(y_test, y_test_pred))

conf_mat = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)
disp.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test, y_test_pred))

y_train_pred = [int(y_pred>0.5) for y_pred in clf.predict(X_train)]
y_test_pred = [int(y_pred>0.5) for y_pred in clf.predict(X_test)]

print("ACCURACY ON TRAIN SET: ", accuracy_score(y_train, y_train_pred))
print("ACCURACY ON TEST SET: ", accuracy_score(y_test, y_test_pred))

conf_mat = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)
disp.plot()
plt.show()

print(classification_report(y_test, y_test_pred))

"""# SHAP EXPLANATIONS"""

!pip install shap

import shap

# SHAP expects model functions to take a 2D numpy array as input, 
# so we define a wrapper function around the original Keras predict function

def prediction_wrapper(X):
  return model.predict([X[:,i] for i in range(X.shape[0])]).flatten()

# select a set of background examples to take an expectation over
background = X_train.iloc[np.random.choice(X_train.shape[0], 100, replace=False)]

explainer = shap.DeepExplainer(
    (model.layers[0].input, model.layers[-1].output), background
)
shap_values = explainer.shap_values(X_test[:].values) 

# print the JS visualization code to the notebook

shap.initjs()
shap.force_plot(
    explainer.expected_value[0], shap_values[0], feature_names=X_train.columns
)

# Negative Sample
shap.initjs()
shap.force_plot(
    explainer.expected_value[0], shap_values[0][1], feature_names=X_train.columns
)

# Positive Sample
shap.initjs()
shap.force_plot(
    explainer.expected_value[0], shap_values[0][10], feature_names=X_train.columns
)

shap.summary_plot(shap_values[0],X_test)
# shap_values

"""# PDP EXPLANATIONS"""

!pip install pdpbox

X.info()

from pdpbox import pdp, info_plots

features = ["Tenure",	"CityTier",	"WarehouseToHome",	"HourSpendOnApp",	"NumberOfDeviceRegistered",	"SatisfactionScore",	"NumberOfAddress", "Complain",	"OrderAmountHikeFromlastYear",	"CouponUsed", "OrderCount", "DaySinceLastOrder", "CashbackAmount", "PreferredLoginDevice_Computer", "PreferredLoginDevice_Mobile Phone", "PreferredLoginDevice_Phone", "PreferredPaymentMode_CC", "PreferredPaymentMode_COD", "PreferredPaymentMode_Cash on Delivery", "PreferredPaymentMode_Credit Card", "PreferredPaymentMode_Debit Card", "PreferredPaymentMode_E wallet", "PreferredPaymentMode_UPI", "Gender_Female", "Gender_Male",	"PreferedOrderCat_Fashion",	"PreferedOrderCat_Grocery",	"PreferedOrderCat_Laptop & Accessory",	"PreferedOrderCat_Mobile",	"PreferedOrderCat_Mobile Phone",	"PreferedOrderCat_Others",	"MaritalStatus_Divorced",	"MaritalStatus_Married",	"MaritalStatus_Single"]

for feature_name in features:
    # Create the data that we will plot
    pdp_goals = pdp.pdp_isolate(model=model, dataset=X_test, model_features=X_train.columns.tolist(), feature=feature_name);

    # plot it
    pdp.pdp_plot(pdp_goals, feature_name);
    plt.show();

"""# CAUSAL ANALYSIS"""

!pip install dowhy
!pip install cdt
!pip install opendatasets

!sudo add-apt-repository ppa:dns/gnu
!sudo apt-get update
!sudo apt install libgsl-dev

!Rscript setup.r

df = pd.read_excel("/content/E Commerce Dataset.xlsx", sheet_name = "E Comm")
df.head()

categorical_cols = ['PreferredLoginDevice', 'PreferredPaymentMode', 'Gender', 'PreferedOrderCat', 'MaritalStatus']

numerical_cols = ['Churn', 'Tenure',  'CityTier',
       'WarehouseToHome',  'HourSpendOnApp',
       'NumberOfDeviceRegistered',  'SatisfactionScore',
        'NumberOfAddress', 'Complain',
       'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount',
       'DaySinceLastOrder', 'CashbackAmount']

from sklearn.impute import SimpleImputer
mean_imputer = SimpleImputer(missing_values = np.nan, strategy = "mean")
df[numerical_cols] = mean_imputer.fit_transform(df[numerical_cols])

# dropping categrical columns
df.drop(categorical_cols + ['CustomerID', "CouponUsed", 'OrderCount', "NumberOfAddress", 'DaySinceLastOrder', 'CashbackAmount'], axis=1, inplace=True)

# Impute NULL valued examples from the dataset
df.isnull().sum()

df.info()

import logging
logging.getLogger("dowhy").setLevel(logging.WARNING)
import warnings
from sklearn.exceptions import DataConversionWarning
warnings.filterwarnings(action='ignore', category=DataConversionWarning)

# Required libraries
import dowhy
from dowhy import CausalModel
import dowhy.datasets
import dowhy.plotter
import seaborn as sns

estimates = []
for i in df.columns:
  l = df.columns.tolist()
  l.remove(i)
  try:
    l.remove('Churn')
  except:pass
  model = CausalModel(
      df,
      common_causes=l,
      treatment = [i],
      outcome = ['Churn']
  )
  try:
    identified_estimand = model.identify_effect()
    estimate = model.estimate_effect(identified_estimand, method_name="backdoor.linear_regression")
    estimates.append([i, estimate.value])
    print(i, "--> churn DONE")
  except:
    estimates.append([i, 0])

for i in estimates:
  if(i[1]!=0):
    print(i[0], "---", i[1])
  else:
    print(i[0], "---", 0)

"""We can observe that Complains cause the most amount of Churn"""

import cdt
import networkx as nx
cdt.SETTINGS.rpath = "/usr/bin/Rscript"
cdt.utils.R.DefaultRPackages.pcalg = True
cdt.utils.R.DefaultRPackages.kpcalg = True
cdt.utils.R.DefaultRPackages.RCIT = True

# Estimated Causal Graph

# LiNGAM Algorithm
model_lingam = cdt.causality.graph.LiNGAM()
graph_lingam = model_lingam.predict(df)

# visualize network
fig=plt.figure(figsize=(25,20))
nx.draw_networkx(graph_lingam, font_size=18, font_color='r')

